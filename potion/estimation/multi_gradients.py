#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Policy gradient stimators
@author: Matteo Papini
"""

import torch
import potion.common.torch_utils as tu
from potion.common.misc_utils import unpack_multi, discount
from potion.common.torch_utils import tensormat, jacobian
from potion.estimation.moments import incr_mean, incr_var
from torch.nn.functional import pad

def multi_gpomdp_estimator(batches, disc, policy, 
                         target_params,
                         behavioral_params,
                         baselinekind='avg', 
                         shallow=False,
                         heuristic="balance"):
    """off-policy G(PO)MDP policy gradient estimator
       
    batches: list of J lists of N_j trajectories each, each generated by the 
        j-th behavioral policy. Each trajectory is a tuple 
        (states, actions, rewards, mask). Each element of the tuple is a 
        tensor where the first dimension is time.
    disc: discount factor
    policy: template policy
    target_params: parameters of the policy to evaluate
    behavioral_params: list of parameters of the behavioral policies; must
        follow the same ordering as batches
    baselinekind: kind of baseline to employ in the estimator. 
        Either 'avg' (average reward, default), 'peters' 
        (variance-minimizing),  or 'zero' (no baseline)
    result: whether to return the final estimate ('mean', default), or the 
        single per-trajectory estimates ('samples')
    shallow: whether to use precomputed score functions (only available
        for shallow policies)
    """
    if shallow:
        return _shallow_multi_gpomdp_estimator(batches, disc, policy, target_params,
                                             behavioral_params, baselinekind, 
                                             heuristic)
    else:
        #Data
        N = len(batch)
        states, actions, rewards, mask, _ = unpack(batch) #NxHxd_s, NxHxd_a, NxH, NxH
        H = rewards.shape[1]
        m = policy.num_params()
        disc_rewards = discount(rewards, disc) #NxH
        
        #Behavioral
        behavioral_params = policy.get_flat()
        behavioral_logps = policy.log_pdf(states, actions) #NxH
        
        #Target
        policy.set_from_flat(target_params)
        target_logps = policy.log_pdf(states, actions) * mask #NxH
        cm_logps = torch.cumsum(target_logps, 1) #NxH
        
        log_iws = torch.cumsum((target_logps - behavioral_logps) * mask, 1) #NxH
        stabilizers, _ = torch.max(log_iws, dim=0, keepdim=True) #NxH
        
        if baselinekind == 'peters':
            jac = jacobian(policy, cm_logps.view(-1)).reshape((N,H,m)) #NxHxm   
            b_num = torch.sum(tensormat((jac * torch.exp(log_iws - stabilizers).unsqueeze(1))**2, 
                                        disc_rewards), 0) #Hxm
            b_den = torch.sum((jac * torch.exp(log_iws - stabilizers).unsqueeze(1))**2, 0) #Hxm
            baseline = b_num / b_den #Hxm
            baseline[baseline != baseline] = 0
            values = disc_rewards.unsqueeze(2) - baseline.unsqueeze(0) #NxHxm
            _samples = torch.sum(tensormat(values * torch.exp(log_iws).unsqueeze(1) * jac, mask), 1) #Nxm
        else:
            if baselinekind == 'avg':
                baseline = torch.mean(disc_rewards, 0) #H
            else:
                baseline = torch.zeros(1) #1
            values = (disc_rewards - baseline) * mask #NxH
            
            _samples = torch.stack([tu.flat_gradients(policy, cm_logps[i,:], 
                                                  values[i,:] * torch.exp(log_iws)[i,:])
                                           for i in range(N)], 0) #Nxm
        
        #restore (behavioral) policy
        policy.set_from_flat(behavioral_params)
        if result == 'samples':
            return _samples #Nxm
        else:
            return torch.mean(_samples, 0) #m

def _shallow_multi_gpomdp_estimator(batches, disc, policy, target_params, 
                         behavioral_params, baselinekind, heuristic):
    with torch.no_grad():
        #Save parameters of template policy
        original_params = policy.get_flat()
        
        J = len(batches) # number of behaviorals
        sizes = [len(batch) for batch in batches]
        N = max(sizes) # largest batch size
        sizes = torch.tensor(sizes) #J
        states, actions, rewards, mask, _ = unpack_multi(batches) #JxNxHxd_s, JxNxHxd_a, JxNxH, JxNxH
        disc_rewards = discount(rewards, disc) #JxNxH
        
        #Target
        policy.set_from_flat(target_params)
        target_logps = policy.log_pdf(states, actions) #JxNxH
        scores = policy.score(states, actions) #JxNxHxm
        
        #Behaviorals
        behavioral_logps = []
        for behav_param in behavioral_params:
            policy.set_from_flat(behav_param)
            behavioral_logps.append(policy.log_pdf(states, actions)) #JxNxH
        behavioral_logps = torch.stack(behavioral_logps) #JxJxNxH
        
        G = torch.cumsum(scores * mask.unsqueeze(-1), 2) #JxNxHxm
        
        diagonals = torch.diagonal(behavioral_logps, dim1=0, dim2=1).reshape(behavioral_logps.shape[1:]) #JxNxH
        log_iws = torch.cumsum((target_logps - diagonals) * mask, -1) #JxNxH
        if heuristic == "balance":
            cum_behavioral_logps = torch.cumsum(behavioral_logps * mask, -1) #JxJxNxH
            stabilizers, _ = torch.max(cum_behavioral_logps, dim=0, keepdim=True)
            unn_coeffs =  torch.einsum("i, ijkh->ijkh", (sizes, torch.exp(cum_behavioral_logps 
                - stabilizers))) #JxJxNxH
            coeffs = unn_coeffs / torch.sum(unn_coeffs, dim=0) #JxJxNxH
            coeffs = torch.diagonal(coeffs, dim1=0, dim2=1).reshape(coeffs.shape[1:]) #JxNxH
        else:
            raise NotImplementedError
            
        if baselinekind == 'peters':
            raise NotImplementedError
            """
            baseline = torch.sum(tensormat(G ** 2, disc_rewards * torch.exp(2*(log_iws - stabilizers))), 0) / \
                            torch.sum(tensormat(G ** 2, torch.exp(2*(log_iws - stabilizers))), 0) #Hxm
            baseline[baseline != baseline] = 0 #removes non-real values
            values = disc_rewards.unsqueeze(2) - baseline.unsqueeze(0) #NxHxm
            """
        else:
            if baselinekind == 'zero':
                baseline = torch.zeros_like(disc_rewards[:,0,:]) #JxH
            elif baselinekind == 'avg':
                baseline = torch.sum(disc_rewards, 1) / torch.sum(mask, 1) #JxH
            else:
                raise NotImplementedError
            baseline[baseline != baseline] = 0 #removes non-real values

        shifted_rewards = (disc_rewards - baseline.unsqueeze(1)) #JxNxH
        result = torch.einsum("abc, abc, abc, abcd -> ad", (coeffs, 
                                                              torch.exp(log_iws), 
                                                              shifted_rewards * mask,
                                                              G)) #Jxm
        result = torch.sum(result / sizes.unsqueeze(-1), 0) #m
        
        #Restore parameters of template policy
        policy.set_from_flat(original_params)
        
        return result
        

def multi_reinforce_estimator(batches, disc, policy, target_params, 
                         behavioral_params, baselinekind="avg", shallow=False,
                         heuristic="balance"):
    if shallow:
        return _shallow_off_reinforce_estimator(batches, disc, policy, target_params, 
                                 behavioral_params, baselinekind, 
                                 heuristic)
    else:
        #data
        N = len(batch)
        states, actions, rewards, mask, _ = unpack(batch) #NxHxm, NxHxd, NxH, NxH
        
        disc_rewards = discount(rewards, disc) #NxH
        rets = torch.sum(disc_rewards, 1) #N
        
        #Behavioral
        behavioral_params = policy.get_flat()
        behavioral_logps = policy.log_pdf(states, actions) #NxH
        
        #Target
        policy.set_from_flat(target_params)
        target_logps = policy.log_pdf(states, actions) * mask #NxH
    
        log_iws = torch.sum((target_logps - behavioral_logps) * mask, 1) #N
        stabilizers, _ = torch.max(log_iws, dim=0, keepdim=True) #N
        
        if baselinekind == 'peters':
            logp_sums = torch.sum(target_logps, 1) #N
            jac = jacobian(policy, logp_sums) #Nxm   
            b_num = torch.sum((jac * torch.exp(log_iws - stabilizers).unsqueeze(1))**2 * rets.unsqueeze(1), 0) #m
            b_den = torch.sum((jac * torch.exp(log_iws - stabilizers).unsqueeze(1))**2, 0) #m
            baseline = b_num / b_den #m
            baseline[baseline != baseline] = 0
            values = rets.unsqueeze(1) - baseline.unsqueeze(0) #Nxm
            _samples = jac * values * torch.exp(log_iws).unsqueeze(1)
        else:
            if baselinekind == 'avg':
                baseline = torch.mean(rets, 0) #1
            else:
                baseline = torch.zeros(1) #1
            baseline[baseline != baseline] = 0
            values = rets - baseline #N
            
            if result == 'mean':
                logp_sums = torch.sum(target_logps, 1)
                grad = tu.flat_gradients(policy, logp_sums, values * torch.exp(log_iws)) / N
                #restore (behavioral) policy
                policy.set_from_flat(behavioral_params)
                return grad
            
            _samples = torch.stack([tu.flat_gradients(policy, target_logps[i,:]) * 
                                                         values[i] * torch.exp(log_iws)[i]
                                           for i in range(N)], 0) #Nxm
        
        #restore (behavioral) policy
        policy.set_from_flat(behavioral_params)
        if result == 'samples':
            return _samples #Nxm
        else:
            return torch.mean(_samples, 0) #m

def _shallow_off_reinforce_estimator(batches, disc, policy, target_params, 
                         behavioral_params, baselinekind, 
                         heuristic):
    with torch.no_grad():
        #Save parameters of template policy
        original_params = policy.get_flat()
        
        J = len(batches) # number of behaviorals
        sizes = [len(batch) for batch in batches]
        N = max(sizes) # largest batch size
        sizes = torch.tensor(sizes, dtype=torch.float) #J
        states, actions, rewards, mask, _ = unpack_multi(batches) #JxNxHxd_s, JxNxHxd_a, JxNxH, JxNxH
        disc_rewards = discount(rewards, disc) #JxNxH
        rets = torch.sum(disc_rewards, dim=2) #JxN
        
        #Target
        policy.set_from_flat(target_params)
        target_logps = torch.sum(policy.log_pdf(states, actions) * mask, -1) #JxN
        scores = policy.score(states, actions) #JxNxHxm
        
        #Behaviorals
        behavioral_logps = []
        for behav_param in behavioral_params:
            policy.set_from_flat(behav_param)
            behavioral_logps.append(torch.sum(policy.log_pdf(states, actions) * mask, -1)) #JxN
        behavioral_logps = torch.stack(behavioral_logps) #JxJxN
        
        G = torch.sum(scores * mask.unsqueeze(-1), -2) #JxNxm
        
        diagonals = torch.diagonal(behavioral_logps, dim1=0, dim2=1).reshape(behavioral_logps.shape[1:]) #JxN
        log_iws = target_logps - diagonals #JxN
        if heuristic == "balance":
            stabilizers, _ = torch.max(behavioral_logps, dim=0, keepdim=True)
            unn_coeffs =  torch.einsum("i, ijk->ijk", (sizes, torch.exp(behavioral_logps 
                - stabilizers))) #JxJxN
            coeffs = unn_coeffs / torch.sum(unn_coeffs, dim=0, keepdim=True) #JxJxN
            coeffs = torch.diagonal(coeffs, dim1=0, dim2=1).reshape(coeffs.shape[1:]) #JxN
        else:
            raise NotImplementedError
            
        if baselinekind == 'peters':
            raise NotImplementedError
            """
            baseline = torch.sum(tensormat(G ** 2, disc_rewards * torch.exp(2*(log_iws - stabilizers))), 0) / \
                            torch.sum(tensormat(G ** 2, torch.exp(2*(log_iws - stabilizers))), 0) #Hxm
            baseline[baseline != baseline] = 0 #removes non-real values
            values = disc_rewards.unsqueeze(2) - baseline.unsqueeze(0) #NxHxm
            """
        else:
            if baselinekind == 'zero':
                baseline = torch.zeros_like(rets) #JxN
            elif baselinekind == 'avg':
                baseline = torch.mean(rets, dim=1, keepdim=True) #JxN
            else:
                raise NotImplementedError
            baseline[baseline != baseline] = 0 #removes non-real values

        shifted_returns = rets - baseline #JxN
        result = torch.einsum("ij, ij, ij, ijk -> ik", (coeffs, 
                                                              torch.exp(log_iws), 
                                                              shifted_returns,
                                                              G)) #Jxm
        result = 1 / sizes @ result #m
        
        #Restore parameters of template policy
        policy.set_from_flat(original_params)
        
        return result     
        
"""Testing"""
if __name__ == '__main__':
    from potion.actors.continuous_policies import ShallowGaussianPolicy as Gauss
    from potion.simulation.trajectory_generators import generate_batch
    from potion.common.misc_utils import seed_all_agent
    import potion.envs
    import gym.spaces
    from potion.estimation.gradients import gpomdp_estimator, reinforce_estimator
    from potion.estimation.offpolicy_gradients import off_reinforce_estimator, off_gpomdp_estimator
    
    env = gym.make('ContCartPole-v0')
    env.seed(0)
    seed_all_agent(0)
    N = 100
    H = 100
    disc = 0.99
    
    #Behavioral
    pol = Gauss(4,1, mu_init=[0.,0.,0.,0.], learn_std=True)
    batch = generate_batch(env, pol, H, N)
    
    #Target
    pi_t = Gauss(4,1, mu_init=[1.,1.,1.,1.], learn_std=True) 
    theta_t = pi_t.get_flat()
    defensive_batch = generate_batch(env, pi_t, H, N//2)
    
    for b in ['zero', 'avg']:
        #Reinforce, shallow, on policy
        on = reinforce_estimator(batch, disc, pol, baselinekind=b, 
                             shallow=True)
       
        off = off_reinforce_estimator(batch, disc, pol, pol.get_flat(),
                                      baselinekind=b, shallow=True)
        
        multi = multi_reinforce_estimator([batch], disc, pol, pol.get_flat(),
                                     [pol.get_flat()],
                                   baselinekind=b,
                                   shallow=True)
        assert torch.allclose(on, off)
        assert torch.allclose(off, multi)
        
        #Reinforce, shallow, off policy
        off = off_reinforce_estimator(batch, disc, pol, theta_t,
                                      baselinekind=b, shallow=True)
        multi = multi_reinforce_estimator([batch], disc, pol, theta_t,
                                          [pol.get_flat()],
                                          baselinekind=b,
                                          shallow=True)
        assert torch.allclose(off, multi)
        
        #Gpomdp, shallow, on policy
        on = gpomdp_estimator(batch, disc, pol, baselinekind=b, 
                             shallow=True)
       
        off = off_gpomdp_estimator(batch, disc, pol, pol.get_flat(),
                                      baselinekind=b, shallow=True)
        
        multi = multi_gpomdp_estimator([batch], disc, pol, pol.get_flat(),
                                     [pol.get_flat()],
                                   baselinekind=b,
                                   shallow=True)
        assert torch.allclose(on, off)
        assert torch.allclose(off, multi)
        
        #Gpomdp, shallow, off policy
        off = off_gpomdp_estimator(batch, disc, pol, theta_t,
                                      baselinekind=b, shallow=True)
        multi = multi_gpomdp_estimator([batch], disc, pol, theta_t,
                                          [pol.get_flat()],
                                          baselinekind=b,
                                          shallow=True)
        assert torch.allclose(off, multi)
        
        #Defensive
        multi = multi_gpomdp_estimator([batch, defensive_batch], disc, pol, theta_t,
                                          [pol.get_flat(), theta_t],
                                          baselinekind=b,
                                          shallow=True)